{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OHUrnb233AG7"
   },
   "source": [
    "# Analizador de sentimientos para comentarios de películas utilizando DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9NI5Cu-n3AHD"
   },
   "source": [
    "## (Reto 01) Paso 1  - Lectura del conjunto de información"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1695838085867,
     "user": {
      "displayName": "Irving Uribe",
      "userId": "05494764447793622080"
     },
     "user_tz": 360
    },
    "id": "zLWkhrzi3AHF"
   },
   "outputs": [],
   "source": [
    "# Importar todas las librerías que se utilizarán\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from numpy import array\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten, Conv1D, LSTM, GlobalMaxPooling1D\n",
    "from keras.layers import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import spacy\n",
    "pln_es = _____._____(\"es_core_news_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r37kXZIX1gUv"
   },
   "outputs": [],
   "source": [
    "# Leemos el archivo y mostramos los primeros elementos para verificar\n",
    "# que sean los correctos\n",
    "movie_reviews = pd._____(\"Peliculas_Reseñas_10000.csv\")\n",
    "# Verificar que no existan valore nulos\n",
    "print(\"Existen valores nulos:\")\n",
    "print(movie_reviews._____().values.any())\n",
    "movie_reviews.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d2fmIyps3AMY",
    "outputId": "a50f0a11-8acc-4a2d-d95b-2ba27cd4391d"
   },
   "outputs": [],
   "source": [
    "# Imprimimos uno de los ejemplos para revisar su escritura\n",
    "movie_reviews[\"reseña\"][300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_oCC_UWE3AMl",
    "outputId": "e761b485-b0f5-493d-800f-82ad95c61395"
   },
   "outputs": [],
   "source": [
    "# Ver la distribución que tenemos de los datos\n",
    "Pos = 0\n",
    "Neg = 0\n",
    "\n",
    "for Sentiment in movie_reviews[\"sentimiento\"]:\n",
    "    if(_____ == 'positive'):\n",
    "        Pos += 1\n",
    "    else:\n",
    "        Neg += 1\n",
    "\n",
    "plt.bar(['Positivas', 'Negativas'], [Pos, Neg], color = ['lime', 'red'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y4KGkolr3AMz"
   },
   "source": [
    "## (Reto 02) Paso 2 - Preprocesamiento de la información"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oVsJVJkj3AM1"
   },
   "outputs": [],
   "source": [
    "# Filtrado de StopWords utilizando NLTK\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('spanish'))\n",
    "\n",
    "X = []\n",
    "\n",
    "removedor_tags = re.compile(r'<[^>]+>')\n",
    "\n",
    "sentences = list(movie_reviews['reseña'])\n",
    "for sen in sentences:\n",
    "\n",
    "    # Filtrado de stopword\n",
    "    for stopword in stop_words:\n",
    "        sentence = sen.replace(\" \" + _____ + \" \", \" \")\n",
    "\n",
    "    # Remover los elementos de HTML (Que aparecen en los comentarios)\n",
    "    sentence = removedor_tags.sub('', sentence)\n",
    "    # Remover espacios múltiples\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "    # Convertir todo a minúsculas\n",
    "    sentence = sentence._____()\n",
    "    # Filtrado de signos de puntuación\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    # Tokenización del resultado (Aplicando el rechazo de tokens descrito)\n",
    "    result = tokenizer.tokenize(_____)\n",
    "    # Agregar al arreglo los textos \"destokenizados\" (Como texto nuevamente)\n",
    "    X.append(TreebankWordDetokenizer().detokenize(result))\n",
    "\n",
    "print(X[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SZt7bcej3ANA",
    "outputId": "3d74ff12-5852-49d9-9eb2-2dd6d05009fc"
   },
   "outputs": [],
   "source": [
    "# Filtrado de más StopWords (Definidas por el usuario)\n",
    "New_StopWords = ['a','acá','ahí','al','algo','algún','alguna','alguno','algunas','algunos','allá','allí','ambos','ante',\n",
    "                 'antes','aquel','aquella','aquello','aquellas','aquellos','aquí','arriba','así','atrás','aun','aunque',\n",
    "                 'bien','cada','casi','como','con','cual','cuales','cualquier','cualquiera','cuan','cuando','cuanto','cuanta',\n",
    "                 'cuantos','cuantas','de','del','demás','desde','donde','dos','el','él','ella','ello','ellas','ellos','en',\n",
    "                 'eres','esa','ese','eso','esas','esos','esta','esto','estas','estos','este','etc','ha','hasta','la','lo','las',\n",
    "                 'los','me','mi','mis','mía','mías','mío','míos','mientras','muy','ni','nosotras','nosotros','nuestra',\n",
    "                 'nuestro','nuestras','nuestros','os','otra','otro','otras','otros','para','pero','pues','que','qué','si','sí',\n",
    "                 'siempre','siendo','sin','sino','so','sobre','sr','sra','sres','sta','su','sus','te','tu','tus','un','una',\n",
    "                 'uno','unas','unos','usted','ustedes','vosotras','vosotros','vuestra','vuestro','vuestras','vuestros','y','ya',\n",
    "                 'yo']\n",
    "\n",
    "\n",
    "X_clear = []\n",
    "# Lematizamos el texto y filtramos las nuevas StopWords\n",
    "for texto in X:\n",
    "    spacy_texto = pln_es(texto)\n",
    "    lemmas = [tok._____.lower() for tok in spacy_texto if not tok.text in _____]\n",
    "    X_clear.append(' '.join(_____))\n",
    "\n",
    "print(\"Textos preprocesados:\")\n",
    "print(X_clear[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6vqnw_tq3ANI",
    "outputId": "27b93a5b-547d-48d8-d180-b0ae2d457ec4"
   },
   "outputs": [],
   "source": [
    "# Creación del vector objetivo (Postivo / Negativo)\n",
    "Sent = movie_reviews['sentimiento']\n",
    "\n",
    "y = [1 if _____ == \"positive\" else 0 for sentimiento in Sent]\n",
    "\n",
    "print(\"Vector de objetivos:\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "53H6PZya3ANa",
    "outputId": "327db06c-3355-47a4-c317-7e738f04e134"
   },
   "outputs": [],
   "source": [
    "# Separación de la información en conjuntos de entrenamiento\n",
    "X_train, X_test, y_train, y_test = _____(np.array(X_clear), np.array(y), test_size=0.20)\n",
    "\n",
    "# Preparamos la capa de embeddingsn(Predefinimos una cantidad de\n",
    "# 5000 palabras consideradas como tokens\n",
    "# SECUENCIAS DE TEXTO\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(_____)\n",
    "\n",
    "# Transforma cada texto en una secuencia de valores enteros\n",
    "X_train = tokenizer._____(X_train)\n",
    "X_test = tokenizer._____(X_test)\n",
    "\n",
    "print(\"Ejemplo de vectores de entrada:\")\n",
    "print(X_train[:3])\n",
    "print(\"\\nEjemplo de salidas\")\n",
    "print(y_train[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aNkjTL9_3AOL",
    "outputId": "1e5b84c5-e81e-4910-d548-b62b6eb97e2f"
   },
   "outputs": [],
   "source": [
    "# Conversión de los conjuntos de entrenamiento a vectores\n",
    "maxlen = 100\n",
    "\n",
    "# Aquellos mayores a 100 son truncados, y los menores les\n",
    "# es replicado su último valor para tener arreglos\n",
    "# bidimensionales del mismo tamaño\n",
    "# A este proceso se le conoce como PADDING\n",
    "X_train = _____(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = _____(X_test, padding='post', maxlen=maxlen)\n",
    "\n",
    "print(\"Ejemplos para el entrenamiento\")\n",
    "print(len(X))\n",
    "print(\"\\nCantidad de elementos en vector:\")\n",
    "print(len(X_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yUPYhr5n3AOe"
   },
   "source": [
    "## Paso 03 -Verificar la coherencia de la información"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ftTbn0Sw3AOo",
    "outputId": "ded89197-f473-4b3e-ba1e-1ca850b9cf6d"
   },
   "outputs": [],
   "source": [
    "# Verificar que la información de entrada\n",
    "print(X_train)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FIb2EbRk3AO3"
   },
   "source": [
    "## (Reto 3) Pasos 4 y 5 - Selección y aplicación del modelo de IA (DNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7fcURwBC3AO4"
   },
   "outputs": [],
   "source": [
    "# Declaración de librerías para manejo de arreglos (Numpy)\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "# Lectura del archivo de embeddings\n",
    "embeddings_dictionary = dict()\n",
    "Embeddings_file = open('Word2Vect_Spanish.txt', encoding=\"utf8\")\n",
    "\n",
    "# Extraemos las características del archivo de embeddings\n",
    "# y las agregamos a un diccionario (Cada elemento es un vextor)\n",
    "for linea in Embeddings_file:\n",
    "    caracts = linea.split()\n",
    "    palabra = caracts[0]\n",
    "    vector = asarray(caracts[1:], dtype='float32')\n",
    "    embeddings_dictionary [palabra] = vector\n",
    "Embeddings_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eIM9OlM-3APQ"
   },
   "outputs": [],
   "source": [
    "# Extraemos la cantidad de palabras en el vocabulario\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Generamos la matriz de embeddings (De entrada)\n",
    "embedding_matrix = zeros((vocab_size, 300))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    # Extraemos el vector de embedding para cada palabra\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    # Si la palbra si existía en el vocabulario\n",
    "    # agregamos su vector de embeddings en la matriz\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KT6ranI23APd",
    "outputId": "5a195ee2-4889-46fc-b516-addf159379a3"
   },
   "outputs": [],
   "source": [
    "# Definimos las capas de nuestro modelo neuronal (DNN)\n",
    "# Definimos que el modelo será secuencial (Y que ingresaremos cada capa en el orden de configuración de la red)\n",
    "model = Sequential()\n",
    "\n",
    "# Definimos los valores para nuestra capa de Embeddings\n",
    "embedding_layer = _____(vocab_size, 300, weights=[_____], input_length=maxlen, trainable=False)\n",
    "\n",
    "# Agregamos al modelo 3 capas (La de Embeddings, una de aplanamiento y una Profunda con una Dense con una salida\n",
    "# y una activación Sigmoindal)\n",
    "model.add(_____)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(_____, activation='sigmoid'))\n",
    "\n",
    "# Definimos métodos de optimización, que métrica se utilizará y como calcular la pérdida\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "# Para ver la configuración que definimos\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C4Y4KiS93APy",
    "outputId": "3651fe18-6237-4559-e296-4f6a6cb39aff"
   },
   "outputs": [],
   "source": [
    "# Utilizamos el método fit para ajustar los datos de nuestro modelo a la configuración que definimos\n",
    "history = model._____(X_train, y_train, batch_size=10, epochs=10, verbose=1, validation_split=0.2)\n",
    "\n",
    "# Utilizamos evaluate para determinar la pérdida y el accuracy de nuestro modelo\n",
    "score = model._____(X_test, y_test, verbose=1)\n",
    "print(\"Test Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SI9occ073AQM",
    "outputId": "58de5185-5b02-4509-9de9-ab527e27b5c3"
   },
   "outputs": [],
   "source": [
    "# Finalmente imprimimos la eficiencia y la pérdida del modelo\n",
    "# época a época para ver su evolución\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8OihjfiZ3AQb"
   },
   "source": [
    "## (Reto 0)4 Paso 6 - Aplicación de una Red Convolucional (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EGBmrPS23AQf",
    "outputId": "41b6429c-b5df-4060-8b7c-d77d159aa2a2"
   },
   "outputs": [],
   "source": [
    "# Declaración de modelo Secuencial\n",
    "model = _____()\n",
    "\n",
    "# Declaración de las capas del modelo convolucional\n",
    "embedding_layer = _____(vocab_size, 300, weights=[embedding_matrix], input_length=maxlen , trainable=False)\n",
    "model.add(_____)\n",
    "model.add(Dense(64, activation='sigmoid'))\n",
    "model.add(Dense(128, activation='sigmoid'))\n",
    "model.add(Dense(64, activation='sigmoid'))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(64, activation='sigmoid'))\n",
    "model.add(Dense(32, activation='sigmoid'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# Impresión de parámetros del modelo\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6tNRIEE-3ASS",
    "outputId": "f0d236cf-2cda-4f17-ad00-d1e2a34e764b"
   },
   "outputs": [],
   "source": [
    "# Utilizamos el método fit para ajustar los datos de nuestro modelo a la configuración que definimos\n",
    "history = model._____(X_train, y_train, batch_size=10, epochs=10, verbose=1, validation_split=0.2)\n",
    "\n",
    "# Utilizamos evaluate para determinar la pérdida y el accuracy de nuestro modelo\n",
    "score = model._____(X_test, y_test, verbose=1)\n",
    "print(\"Test Score:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FOVK9iw73ASY",
    "outputId": "884f5591-2aa6-46c2-cb45-26d853a33cd4"
   },
   "outputs": [],
   "source": [
    "# Finalmente imprimimos la eficiencia y la pérdida del modelo\n",
    "# época a época para ver su evolución\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','test'], loc = 'upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','test'], loc = 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Ok7Ls5O3ATC"
   },
   "source": [
    "## (Reto 05) Paso 6.1 - Red LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IxdrDQ303ATD",
    "outputId": "d46421ec-b247-4add-8747-e56149d6a827"
   },
   "outputs": [],
   "source": [
    "# Declaración de modelo Secuencial\n",
    "model = Sequential()\n",
    "\n",
    "# Declaración de las capas del modelo LSTM\n",
    "embedding_layer = _____(vocab_size, 300, weights=[embedding_matrix], input_length=maxlen , trainable=False)\n",
    "model.add(_____)\n",
    "model.add(LSTM(10))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Impresión de parámetros del modelo\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OXA-RgoY3AVj",
    "outputId": "fa24854d-6706-409e-f84b-118a1c85e194"
   },
   "outputs": [],
   "source": [
    "# Utilizamos el método fit para ajustar los datos de nuestro modelo a la configuración que definimos\n",
    "history = model._____(X_train, y_train, batch_size=10, epochs=10, verbose=1, validation_split=0.2)\n",
    "\n",
    "# Utilizamos evaluate para determinar la pérdida y el accuracy de nuestro modelo\n",
    "score = model._____(X_test, y_test, verbose=1)\n",
    "print(\"Test Score:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ekg6yBky3AVr",
    "outputId": "dc0173ec-5ae1-4e75-b803-e13cf12daab8"
   },
   "outputs": [],
   "source": [
    "# Finalmente imprimimos la eficiencia y la pérdida del modelo\n",
    "# época a época para ver su evolución\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a1mi0MWX3AVw"
   },
   "source": [
    "## Paso 7 - Probador de información"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2mLiZxGL3AVx",
    "outputId": "4dccbf4e-874f-474d-a6f4-cd1ee2920fe7"
   },
   "outputs": [],
   "source": [
    "utterance = \"Esta es una película de verdad decepcionante, aburrida y que realmente no puedes disfrutar\"\n",
    "#utterance = \"Esta es una gran película, me encantó, es de lo mejor del año\"\n",
    "\n",
    "sequence = tokenizer.texts_to_sequences([utterance])\n",
    "padded = pad_sequences(sequence, padding='post', maxlen=maxlen)\n",
    "\n",
    "# Resultado de la evaluación\n",
    "print(\"Resultado del modelo\")\n",
    "model.predict(padded)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

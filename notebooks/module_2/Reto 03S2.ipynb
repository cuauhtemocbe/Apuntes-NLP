{"cells":[{"cell_type":"markdown","metadata":{"id":"CEYn3vnCz4he"},"source":["# Reto 03S2: Extracción y reducción de características"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"IROcegnET3CT"},"outputs":[],"source":["# Importar el núcleo de trabajo en Español\n","import spacy\n","\n","nlp_es = spacy.load(\"es_core_news_md\")"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"ehdK1RsddhXY"},"outputs":[],"source":["# Frases de películas \n","Frases = {'Frase1': nlp_es(u'Hasta la vista, baby.'),\n","          'Frase2': nlp_es(u'La locura es como la gravedad, solo se necesita un empujoncito.'),\n","          'Frase3': nlp_es(u'Algunos hombres sólo quieren ver el mundo arder.'),\n","          'Frase4': nlp_es(u'¿Por qué caemos Bruce? Para aprender a levantarnos.'),\n","          'Frase5': nlp_es(u'¿De qué sirve confesarme, si no me arrepiento?.'),\n","          'Frase6': nlp_es(u'¡Soy el rey del mundo!'),\n","          'Frase7': nlp_es(u'Todos tenemos el amor que creemos merecer.'),\n","          'Frase8': nlp_es(u'Todos flotan, y tú también flotarás.'),\n","          'Frase9': nlp_es(u'Hay tres maneras de hacer las cosas, la correcta, la incorrecta y la mía.'),\n","          'Frase10': nlp_es(u'No. No lo intentes. Hazlo, o no lo hagas, pero no lo intentes.')}"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"JQ61DijtkfBc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Vocabulario: \n","{'hasta': 0, 'la': 1, 'vista': 2, ',': 3, 'baby': 4, '.': 5, 'locura': 6, 'es': 7, 'como': 8, 'gravedad': 9, 'solo': 10, 'se': 11, 'necesita': 12, 'un': 13, 'empujoncito': 14, 'algunos': 15, 'hombres': 16, 'sólo': 17, 'quieren': 18, 'ver': 19, 'el': 20, 'mundo': 21, 'arder': 22, '¿': 23, 'por': 24, 'qué': 25, 'caemos': 26, 'bruce': 27, '?': 28, 'para': 29, 'aprender': 30, 'a': 31, 'levantarnos': 32, 'de': 33, 'sirve': 34, 'confesarme': 35, 'si': 36, 'no': 37, 'me': 38, 'arrepiento': 39, '¡': 40, 'soy': 41, 'rey': 42, 'del': 43, '!': 44, 'todos': 45, 'tenemos': 46, 'amor': 47, 'que': 48, 'creemos': 49, 'merecer': 50, 'flotan': 51, 'y': 52, 'tú': 53, 'también': 54, 'flotarás': 55, 'hay': 56, 'tres': 57, 'maneras': 58, 'hacer': 59, 'las': 60, 'cosas': 61, 'correcta': 62, 'incorrecta': 63, 'mía': 64, 'lo': 65, 'intentes': 66, 'hazlo': 67, 'o': 68, 'hagas': 69, 'pero': 70}\n"]}],"source":["# Construcción del vocabulario (Diccionario de características)\n","# El diccionario \"Vocabulario\" almacena cada uno de los tokens diferentes, con\n","# su respectivo valor de frecuencia o repetición\n","# El arreglo \"Voc\" almacena los nombres de los tokens, permitiendo conocer\n","# la posición de cada uno (index) ya que los diccionarios no soportan el uso\n","# de index\n","Vocabulario = {}\n","Voc = []\n","\n","i = 0\n","for Frase in Frases.values():\n","    for Token in Frase:\n","        if(Token.text.lower() not in Vocabulario):\n","            Vocabulario[Token.text.lower()] = i\n","            Voc.append(Token.text)\n","            i += 1\n","\n","print(\"Vocabulario: \")\n","print(Vocabulario)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"ecZi6yep_31Q"},"outputs":[{"name":"stdout","output_type":"stream","text":["['Frase1', 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","['Frase2', 0, 2, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","['Frase3', 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","['Frase4', 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","['Frase5', 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","['Frase6', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","['Frase7', 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","['Frase8', 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","['Frase9', 0, 3, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n","['Frase10', 0, 0, 0, 2, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 2, 1, 1, 1, 1]\n"]}],"source":["# Asignación de frecuencia de características en la matriz, e impresión\n","Caract = []\n","\n","for Label in Frases.keys():\n","    Caract.append([Label] + [0]*len(Vocabulario))\n","\n","F = 0\n","# Llenado de la frecuencia de cada elemento del vocabulario por cada frase\n","for Frase in Frases.values():\n","    for token in Frase:\n","        # Lista bidimensional [Frase][Token]\n","        # Añadir el índice de frase y token correspondiente para sumar \"1\" su frecuencia\n","        Caract[F][Vocabulario[token.text.lower()] + 1] += 1\n","    F += 1\n","\n","for Car in Caract:\n","    print(Car)"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"ogxwgRs5THC0"},"outputs":[{"name":"stdout","output_type":"stream","text":["[1, 3, 1, 6, 1, 9, 1, 1, 1, 1]\n"]}],"source":["# Conteo de apariciones de cada característica (Global)\n","Repeticiones = []\n","\n","for i in range(len(Caract)):\n","    N = 0\n","    for j in range(len(Caract)):\n","        if(Caract[j][i+1] != 0):\n","            N += 1\n","      # Agregar en el arreglo el valor obtenido \n","    Repeticiones.append(N)\n","\n","print(Repeticiones)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AU6F1JCUbCGv"},"outputs":[],"source":["# Generación del nuevo diccionario e impresión de características rechazadas\n","# Utilizar \"Vocabulario2\" como el nuevo vocabulario de palabras sin repetir\n","# y sus respectivos valores de frecuencia\n","# Voc2 ayuda a conocer la posición de cada token (Index)\n","Vocabulario2 = {}\n","Voc2 = []\n","Eliminadas = []\n","n = 0\n","\n","for r in range(len(Repeticiones)):\n","    # Solo agregar las que aparecen solo una vez\n","    if(Repeticiones[r] == 1):\n","        # Agregar la característica (En minúsculas)\n","        Vocabulario2._____(Voc[r].lower(), n)\n","        Voc2._____(Voc[r])\n","        n += 1\n","    else:\n","        Eliminadas._____(Voc[r].lower())\n","\n","print(\"Nuevo vocabulario:\")\n","print(Vocabulario2)\n","print(\"\\nCaracterísticas rechazadas:\")\n","print(Eliminadas)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-hx8vkrdfH0v"},"outputs":[],"source":["# Nueva matriz de características\n","Caract2 = []\n","\n","for Label in Frases.keys():\n","    Caract2._____([Label] + [0]*len(Vocabulario2))\n","\n","F2 = 0\n","for Frase in Frases.values():\n","    for token in Frase:\n","        if(str(token) in Voc2):\n","            # Arreglo bidimensional [Frase][Token]\n","            # Añadir el índice de frase y token correspondiente para sumar \"1\" su frecuencia\n","            Caract2[F2][Vocabulario2[token._____.lower()]+1] += 1\n","    F2 += 1\n","\n","for Car2 in Caract2:\n","    print(Car2)"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Reto 03S2.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}

{"cells":[{"cell_type":"markdown","metadata":{"id":"ZGTi3ZIdH9GW"},"source":["## Paso 01 - Lectura del conjunto de información"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WxzlCeaWFMca"},"outputs":[],"source":["# Importar las librerías de que nos ayudarán a obtener la información, organizarla y procesarla\n","import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K4BDdkcYT2X8"},"outputs":[],"source":["# Lectura del archivo que contiene el corpus con la información clasificada\n","# y preprocesada (Ham VS Spam)\n","df2 = pd.read_csv('Spamless_DataSet_N.csv')\n","df2.head(20)\n","\n","# Referencia para la lectura de archivos en: https://codeday.me/es/qa/20190412/477724.html"]},{"cell_type":"markdown","metadata":{"id":"39-gKrypIV_K"},"source":["## Paso 02 - Verificación y validación de la información"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VDTEjN9zUr4Y"},"outputs":[],"source":["# Para detectar si tenemos filas vacías\n","print(\"Tamaño del data frame: \" + str(len(df2)))\n","print(\"Cantidad de filas vacías:\")\n","print(df2.isnull().sum())\n","\n","# Eliminamos las filas vacías y vemos el tamaño del nuevo df\n","df2 = df2.dropna()\n","df2 = df2[df2['mensaje'] != \"\"]\n","print(\"\\nTamaño del data frame sin vacíos: \" + str(len(df2)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SBfWygp_VO78"},"outputs":[],"source":["# Verificar cuales son las clasificaciones que tenemos en nuestro DataSet\n","print(\"Clases que tenemos en el DataSet:\")\n","print(df2['clase'].unique())\n","\n","# Revisamos la cantidad de ejemplos que tenemos por cada clase\n","print(\"\\nCantidad de ejemplos que tenemos por clase:\")\n","print(df2['clase'].value_counts())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z01qApnCiF4l"},"outputs":[],"source":["# Ploteamos la cantidad de ejemplos que tenemos de cada clase\n","# para ver si esta se encuentra balanceada o no\n","import matplotlib.pyplot as plt\n","\n","Totales = df2['clase'].value_counts()\n","plt.bar(['Spam', 'NoSpam'], Totales)\n","plt.xticks(rotation = 45)\n","plt.title('Cantidad de ejemplos de cada clase')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"5y-Ad-FUQdGW"},"source":["## Paso 04 - Buscar relaciones en la información"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rb6tE8gliwua"},"outputs":[],"source":["# Para analizar la información, ploteamos un histograma\n","# que nos indica la frecuencia de caracteres tanto en\n","# Ham como en Spam para revisar su comportamiento\n","\n","Spams = []\n","NoSpams = []\n","for clase, mensaje in zip(df2['clase'], df2['mensaje']):\n","    if clase == 'Spam':\n","        Spams.append(len(mensaje))\n","    else:\n","        NoSpams.append(len(mensaje))\n","\n","\n","plt.hist(Spams, bins=15)\n","plt.hist(NoSpams, bins=15)\n","plt.legend(['Spam', 'No Spam'])\n","plt.xlabel('Cantidad de Caracteres en los ejemplos')\n","plt.ylabel('Total de ejemplos')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qM7mv7_7anFe"},"outputs":[],"source":["# Importamos el núcleo de trabajo para hacer la tokenización\n","import spacy.cli\n","spacy.cli.download('es_core_news_sm')\n","nlp_es = spacy.load('es_core_news_sm')\n","\n","# Gráfica de los tokens más utilizados para Spam\n","Palabras_Spam = {}\n","\n","for clase, mensaje in zip(df2['clase'], df2['mensaje']):\n","    if clase == 'Spam':\n","        for token in nlp_es(mensaje):\n","            if(Palabras_Spam.get(token.text) == None):\n","                Palabras_Spam.setdefault(token.text, 1)\n","            else:\n","                Palabras_Spam[token.text] += 1\n","\n","Palabras_Spam2 = {}\n","for clave, valor in zip(Palabras_Spam.keys(), Palabras_Spam.values()):\n","    if(valor >= 5):\n","        Palabras_Spam2.setdefault(clave, valor)\n","\n","plt.figure(figsize=(10,6))\n","plt.bar(Palabras_Spam2.keys(), Palabras_Spam2.values())\n","plt.title('Palabras más importantes para Spam')\n","plt.xticks(rotation=80)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1PFw9RbIanBl"},"outputs":[],"source":["# Gráfica de los tokens más utilizados para No Spam\n","Palabras_NoSpam = {}\n","\n","for clase, mensaje in zip(df2['clase'], df2['mensaje']):\n","    if clase == 'NoSpam':\n","        for token in nlp_es(mensaje):\n","            if(Palabras_NoSpam.get(token.text) == None):\n","                Palabras_NoSpam.setdefault(token.text, 1)\n","            else:\n","                Palabras_NoSpam[token.text] += 1\n","\n","Palabras_NoSpam2 = {}\n","for clave, valor in zip(Palabras_NoSpam.keys(), Palabras_NoSpam.values()):\n","    if(valor >= 2):\n","        Palabras_NoSpam2.setdefault(clave, valor)\n","\n","plt.figure(figsize=(10,6))\n","plt.bar(Palabras_NoSpam2.keys(), Palabras_NoSpam2.values())\n","plt.title('Palabras más importantes para NoSpam')\n","plt.xticks(rotation=80)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"A7wK3-yEQrj7"},"source":["## Paso05 - Construcción del modelo de NLP"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"unryiPTmO9QD"},"outputs":[],"source":["# Separar la información (Dataset) en conjuntos de entrenamiento y validación\n","\n","# Importar la librería de Sklearn\n","from sklearn.model_selection import train_test_split\n","\n","# Definimos nuestra información (Nombre de la clase (Y) y valor (X))\n","X = df2['mensaje'] # Entrada (Información que tenemos)\n","Y = df2['clase'] # Salidas (Respuestas deseadas para cada texto de entrada)\n","\n","# Segmentamos la información en conjuntos de entrenamiento y de validación (80 / 20)\n","X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wMEa9UraTSka","scrolled":true},"outputs":[],"source":["# Se importa la librería para extraccion de características y vectorización\n","# de los ejemplos usando la función CountVectorizer()\n","# (Permite conocer información de los daros de entrenamiento)\n","from sklearn.feature_extraction.text import CountVectorizer\n","count_vect = CountVectorizer()\n","\n","#  Con esta línea se construye un diccionario (Vocabulario de palabras) y se\n","# cuenta el número de palabras que hay para cada elemento del diccionario\n","X_train_counts = count_vect.fit_transform(X_train)\n","\n","# (Shape) El primer número representa los mensajes, y el segundo el vocabulario usado\n","X_train_counts.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"axp9LE877oim"},"outputs":[],"source":["# Se crea un Pipeline para asignar la secuencia de procesos\n","from sklearn._____ import Pipeline\n","from sklearn._____.text import TfidfVectorizer\n","from sklearn._____ import LinearSVC\n","\n","# En el Pipeline se mete en un arreglo la secuencia de pasos que se desea\n","# seguir, o elementos que se desea enviar, en este caso 1) Vector de Tf-idf\n","# 2) El modelo LinearSVC\n","# En esta línea se hace la vectorización y se ejecuta el clasificador en un solo paso\n","clasificador_Texto = Pipeline([('tfidf', TfidfVectorizer()),('clf', LinearSVC())])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fI2JbgOa7ofy"},"outputs":[],"source":["# Visualizamos los parámetros de nuestro modelo (De referencia)\n","print(clasificador_Texto.get_params().keys())\n","\n","# Podemos cambiar algunos de los parámetros como por ejempplo, uso de minúsculas\n","# filtrado de stopwords y máximo de características a usar\n","clasificador_Texto.set_params(tfidf__lowercase=True, tfidf__stop_words=['de', 'para'], tfidf__max_features=None)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vjswMTZX7ob5"},"outputs":[],"source":["clasificador_Texto.fit(_____, _____) # Entrasas VS Salidas\n","Predicciones = clasificador_Texto.predict(_____) # Usar conjunto de pruebas\n","print(X_test)\n","print(Predicciones)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xian3OLy7oad"},"outputs":[],"source":["clasificador_Texto.predict([\"Gracias por cofirmar tu asistencia\",\n","                            \"Hola usuario, tenemos una oferta para tí\",\n","                            \"Solo por hoy, el precio ha bajado\",\n","                            \"Esperamos que te encuentras bien, ya hemos hecho la activación de tu cuenta\",\n","                            \"Porqué no revisas estos ofertones??\",\n","                            \"Mantequilla\"])"]},{"cell_type":"markdown","metadata":{"id":"YHQgl3tb-rIi"},"source":["## Matriz de confusión"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nq4Q6FFdZjON"},"outputs":[],"source":["# Matriz de confusión y Métricas de evaluación del modelo\n","from sklearn.metrics import confusion_matrix, classification_report\n","from sklearn import metrics\n","\n","# Impresión de matriz de confusión\n","print(\"Matriz de confusión:\")\n","print(confusion_matrix(_____, Predicciones))\n","\n","# Impresión de procentaje de Accuracy del modelo\n","print(\"\\nAccuracy del modelo: \")\n","print(metrics.accuracy_score(_____, Predicciones))\n","\n","# Impresión de las métricas para el modelo\n","print(\"\\nMétricas de evaluación:\")\n","print(classification_report(_____, Predicciones))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nantp6ncbM2p"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"}},"nbformat":4,"nbformat_minor":0}